{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "# Machine Learning Foundation\n",
    "\n",
    "## Course 6, Part d: Smoothing DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Time series data is data that is measured at equally-spaced intervals, and can be decomposed into trend, seasonality, and residuals. Many time series models require the data to be stationary in order to make forecasts.  In this notebook, we'll build upon these concepts by exploring another important concept called **smoothing**.\n",
    "\n",
    "# Learning Outcomes\n",
    "You should walk away from this demonstration with:\n",
    "1. A practical understanding of smoothing and why it is necessary.\n",
    "2. Several common smoothing techniques.\n",
    "3. A basic understanding of how to smooth time series data with Python and generate forecasts.\n",
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ef5768388ffb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcolorsetup\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_palette\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpalette\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'data'"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import statsmodels as ss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import warnings\n",
    "os.chdir('data')\n",
    "from colorsetup import colors, palette\n",
    "sns.set_palette(palette)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Smoothing \n",
    "\n",
    "In this lesson, we will explore the idea of smoothing a time series. Specifically, we'll discuss what smoothing is and why it is necessary.\n",
    "\n",
    "As before, we'll generate datasets from scratch to help you grasp the advantages of smoothing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Smooothing?\n",
    "Any data collection process is subject to noise. Oftentimes this noise can obscure useful patterns. Smoothing is a well-known and oft used technique to extract those patterns. \n",
    "\n",
    "Smoothing comes in two flavors:\n",
    "1. Simple \n",
    "2. Exponential \n",
    "\n",
    "We'll explore these concepts more deeply in a moment. However, before we do, let's kick things off with the stationary data we discussed in the last lesson on stationary time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# data\n",
    "time = np.arange(100)\n",
    "stationary = np.random.normal(loc=0, scale=1.0, size=len(time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now you should know that you should always visually inspect your time series with a run-sequence plot. \n",
    "> Again, we'll create a *run_sequence_plot* function to make plotting throughout this tutorial less cumbersome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sequence_plot(x, y, title, xlabel=\"time\", ylabel=\"series\"):\n",
    "    plt.plot(x, y, 'k-')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(alpha=0.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sequence_plot(time, stationary, \n",
    "                  title=\"Stationary TS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we can discuss the two types of smoothing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Common Smoothing Techniques\n",
    "This may surprise you but there are many techniques for smoothing data. \n",
    "\n",
    "The ones we'll discuss in this tutorial are:\n",
    "1. Simple Smoothing\n",
    "2. Moving Average Smoothing\n",
    "3. Exponential Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple Smoothing\n",
    "Simple smoothing is the most obvious place to start. \n",
    "\n",
    "Consider the stationary data above. How would you go about extracting information from this series? \n",
    "\n",
    "Naturally, your inclination may be to calculate the mean and use that as an estimate to represent the series. \n",
    "\n",
    "Let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find mean of series\n",
    "stationary_time_series_avg = np.mean(stationary)\n",
    "\n",
    "# create array composed of mean value and equal to length of time array\n",
    "sts_avg = np.full(shape=len(time), fill_value=stationary_time_series_avg, dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sequence_plot(time, stationary,\n",
    "                  title=\"Stationary Data w/Mean\")\n",
    "plt.plot(time, sts_avg, 'r', label=\"mean\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a way to measure how far off our estimate is from reality. A common way to do this with continuous data is by calculating Mean Squared Error (MSE). \n",
    "\n",
    "The formula for MSE is:  $MSE = \\frac{1}{n}\\sum_{i=0}^{n} (observed_{i} - estimate_{i})^2$\n",
    "\n",
    "Let's walk through a quick example to make sure you understand what's happening.\n",
    "\n",
    "Say we an array of observed values [0, 1, 2, 3] and our estimate is the array [1, 1, 1, 1]. \n",
    "\n",
    "We calculate MSE like this: $(0-1)^{2} + (1-1)^{2} + (2-1)^{2} + (3-1)^{2}$ which is 6. \n",
    "\n",
    "Say we had another estimate [0, 0, 0, 0]. Let's calculate MSE once again.\n",
    "\n",
    "The MSE is simply $0^{2} + 1^{2} + 2^{2} + 3^{2}$ which is 14. \n",
    "\n",
    "Having MSE gives us a way to compare different estimates to see which is best. In this case, the array of 1's is a far more accurate representation of the data than the array of 0's. We know this because the MSE is lower for the array of 1's. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This idea of measuring how a model is performing crops up throughout machine learning and we will see its power later in this tutorial.\n",
    "\n",
    "Now let's create a function to calculate MSE and discuss smoothing when a trend is present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSE Function\n",
    "Let's create a Mean Squared Error function that we can use going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(observations, estimates):\n",
    "    '''\n",
    "    INPUT:\n",
    "        observations - numpy array of values indicating observed values\n",
    "        estimates - numpy array of values indicating an estimate of values\n",
    "    OUTPUT:\n",
    "        Mean Square Error value\n",
    "    '''\n",
    "    # check arg types\n",
    "    assert type(observations) == type(np.array([])), \"'observations' must be a numpy array\"\n",
    "    assert type(estimates) == type(np.array([])), \"'estimates' must be a numpy array\"\n",
    "    # check length of arrays equal\n",
    "    assert len(observations) == len(estimates), \"Arrays must be of equal length\"\n",
    "    \n",
    "    # calculations\n",
    "    difference = observations - estimates\n",
    "    sq_diff = difference ** 2\n",
    "    mse = sum(sq_diff)\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the **mse** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = mse(np.array([0, 1, 2, 3]), np.array([0, 0, 0, 0]))\n",
    "print(zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = mse(np.array([0, 1, 2, 3]), np.array([1, 1, 1, 1]))\n",
    "print(ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend = (time * 2.75) + stationary\n",
    "run_sequence_plot(time, trend,\n",
    "                  title=\"Nonstationary Data w/Trend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear in the plot above that there is an upward trend. Suppose we take the mean of the series again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find mean of series\n",
    "trend_time_series_avg = np.mean(trend)\n",
    "\n",
    "# create array of mean value equal to length of time array\n",
    "trend_avg = np.full(shape=len(time), fill_value=trend_time_series_avg, dtype='float')\n",
    "\n",
    "run_sequence_plot(time, trend,\n",
    "                  title=\"Trend Data w/Mean\")\n",
    "plt.plot(time, trend_avg, 'r', label=\"mean\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Do you think this would be a good way to extract information if a trend is present?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** No, the plot clearly shows this is problematic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we must find another way to approximate the underlying pattern inherent in the data. There are many ways to capture that trend but let's keep things simple and build up. Let's start with something called a moving average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Moving Average Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to understand moving average is by example. \n",
    "\n",
    "Say we have the values [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]. \n",
    "\n",
    "The first step is to select a window size. We'll choose a size of three arbitrarily. So what happens is we compute the average for the first three values and store the result. We then slide the window over one spot and calculate the average of the next three values. We continue this process until we reach the final observed value. \n",
    "\n",
    "Therefore, the window begins with the values [0, 1, 2]. The mean of that sequence is 1 so we store that value [1]. We slide the window over one place to [1, 2, 3] and compute the mean which is 2. We add that value to storage so we have [1, 2]. We continue this process until we reach the last window of size three [7, 8, 9]. We conclude by calculating the mean and storing the result. The result is an array of smoothed values: [1, 2, 3, 4, 5, 6, 7, 8]. \n",
    "\n",
    "Here's the cool part. Let's compare MSE for the two possibilities we've discussed thus far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data for arrays discussed above\n",
    "dummy_data = np.arange(10)\n",
    "dummy_avg = np.mean(dummy_data)\n",
    "dummy_avg_array = np.full(shape=len(dummy_data), fill_value=dummy_avg, dtype='float')\n",
    "\n",
    "# calc MSE's\n",
    "dummy_simple_average_mse = mse(dummy_data, dummy_avg_array)\n",
    "dummy_smoothed_average_mse = mse(dummy_data[1:-1], np.arange(1, 9))\n",
    "\n",
    "# print results\n",
    "print(\"MSE\")\n",
    "print(\"-\" * 12)\n",
    "print(\"Simple: \", dummy_simple_average_mse)\n",
    "print(\"Smoothed: \", dummy_smoothed_average_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we start to see the power of smoothing. A simple average of the trend data resulted in an abysmal MSE of 82.5 whereas a smoothed version of this contrived example produced a perfect estimation (MSE of 0). \n",
    "> An MSE of 0 is nearly always impossible unless you have toy data like this.\n",
    "\n",
    "Let's create a moving average function to make this process easier moving forward.\n",
    "> The guts of this function were taken from [this](https://stackoverflow.com/questions/14313510/how-to-calculate-moving-average-using-numpy#14314054) stackoverflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(observations, window=3, forecast=False):\n",
    "    '''returns the smoothed version of an array of observations.'''\n",
    "    cumulative_sum = np.cumsum(observations, dtype=float)\n",
    "    cumulative_sum[window:] = cumulative_sum[window:] - cumulative_sum[:-window]\n",
    "    if forecast:\n",
    "        return np.insert(cumulative_sum[window - 1:] / window, 0, np.zeros(3))\n",
    "    else:\n",
    "        return cumulative_sum[window - 1:] / window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now let's calculate the moving average of the trend data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_trend = moving_average(trend, window=3, forecast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sequence_plot(time, trend,\n",
    "                  title=\"Trend Data w/Smoothing\")\n",
    "plt.plot(time[1:], np.insert(smoothed_trend,0,0), 'r', label=\"smooth\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad at all! You can see the smoothed version picks up the trend rather nicely, far better than the simple average. Now we're getting somewhere.\n",
    "\n",
    "Let's try the same thing with seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonality = 10 + np.sin(time) * 10\n",
    "smoothed_seasonality = moving_average(seasonality, window=3, forecast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sequence_plot(time, seasonality,\n",
    "                  title=\"Seasonality Data w/Smoothing\")\n",
    "plt.plot(time[1:-1], smoothed_seasonality, 'r', label=\"smooth\")\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's far from perfect but clearly picks up the inherent pattern contained within this seasonal pattern.\n",
    "\n",
    "Lastly, let's see how moving average handles trend, seasonality, and a bit of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_seasonality = trend + seasonality + stationary\n",
    "smoothed_trend_seasonality = moving_average(trend_seasonality, window=3, forecast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sequence_plot(time, trend_seasonality,\n",
    "                  title=\"Trend, Seasonality, & Noisy Data w/Smoothing\")\n",
    "plt.plot(time[1:-1], smoothed_trend_seasonality, 'r', label=\"smooth\")\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall this method is doing a decent job on these toy datasets. You can see it's picking up key patterns in the data. However, we've been weighting all observations equally. If you think about time series data, though, you may quickly realize that the most recent observations tend to impact the current to a larger degree than older ones. \n",
    "\n",
    "This leads us to another averaging strategy: exponential smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exponential Smoothing\n",
    "Exponential smoothing is a way to weight observations differently. Specifically, recent observations are weighted moreso than more distant ones. This makes intuitive sense. Think back to the stock market example that we discussed in past lessons. In general, it has been observed that today's price is a good predictor for tomorrow's price. Using an equally weighted smoothing process like the one introduced earlier would dilute that. However, by applying unequal weights to past observations, we can control how much each affects the future forecast. \n",
    "\n",
    "Let's make this concrete with an example. \n",
    "\n",
    "Say we have the values [1, 2, 4, 16, 256]. \n",
    "\n",
    "Instead of pulling out the inherent pattern within a series, let's use these smoothing functions to create forecasts. In other words, we'll apply the smoothing process and use the resulting value as the next data point.\n",
    "\n",
    "With an equally weighted moving average discussed earlier we'd get a smoothed result that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.array([1, 2, 4, 8, 16, 32, 64])\n",
    "ma_smoothed_vals = moving_average(values, window=3, forecast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(len(values))\n",
    "run_sequence_plot(t, values,\n",
    "                  title=\"Nonlinear Data w/MA Smoothing\")\n",
    "plt.plot(t, ma_smoothed_vals[:-1], 'r', label=\"smooth\")\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's weight recent values more aggressively than older values. \n",
    "\n",
    "We'll use a simple exponential weighting whereby $w_{1} + w_{2}^2 + w_{3}^3 = 1$. \n",
    "\n",
    "This means $w_{1}$ is ~0.543, $w_{2}$ is ~0.294, and $w_{3}$ is ~0.160."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewma(observations, forecast=True):\n",
    "    '''returns the exponentially weighted smoothed version of an array of observations.'''\n",
    "    weights = np.array([0.160, 0.294, 0.543])\n",
    "    output = np.zeros_like(observations, dtype='float')\n",
    "    for i, _ in enumerate(observations):\n",
    "        if (i == 0) or (i == 1) or (i == len(observations) - 1):\n",
    "            pass\n",
    "        else:\n",
    "            output[i] = np.dot(observations[i-2:i+1], weights)\n",
    "    if forecast:\n",
    "        return np.insert(output, 0, 0)\n",
    "    else:\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewma_smoothed_vals = ewma(values, forecast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sequence_plot(t, values,\n",
    "                  title=\"Nonlinear Data w/MA Smoothing\")\n",
    "plt.plot(t, ma_smoothed_vals[:-1], 'r', label=\"moving ma\")\n",
    "plt.plot(t, ewma_smoothed_vals[:-1], 'b', label=\"exp ma\")\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see both moving average techniques have trouble keeping pace with such an aggressive trend. There's significant lag. This gives us a glimpse into why using moving average techniques alone to detect and forecast trend can be problematic. One thing that is clear, however, is that the exponential weighting does a better job following the trend. \n",
    "\n",
    "#### Setting Weights\n",
    "I chose the weights above for explanatory purposes but there are numerous methods to find the optimal weighting scheme. A full discussion is far beyond the scope of this tutorial. Nonetheless, it is helpful that you be acclimated to the formulation and possible initialization strategies.\n",
    "\n",
    "The formulation is this: $S_{t} = \\alpha*y_{t-1} + \\alpha(1-\\alpha)S_{t-1} + \\alpha(1-\\alpha)^2S_{t-1} +$ ...\n",
    "\n",
    "$S_{t}$ is the smoothed value at time *t*  \n",
    "$\\alpha$ is a smoothing constant  \n",
    "$y_{t-1}$ is the value of the series at time *t-1*\n",
    "\n",
    "#### Initialization\n",
    "There are many initialization strategies. A simple one is to set $S_2$ equal to $y_1$. Another strategy is to find the mean of the first couple of observations. Once you've initialized, you simply use the update rule above to calculate all values. \n",
    "\n",
    "Thankfully, you'll never have to actually go through this arduous process. There are great methods in **statsmodels** that will do all this heavy lifting for you. \n",
    "\n",
    "#### Setting Alpha\n",
    "One last comment: choosing the optimal value for $\\alpha$ is also taken care of for you by **statsmodels**. A solver uses a metric like MSE to find an $\\alpha$ that minimizes that metric. Therefore, you rarely have to choose this value yourself, but at least you have a basic idea as to what's happening under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Three Types of Exponential Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three key exponential smoothing techniques you need to be aware of:\n",
    "1. Single Exponential Smoothing - no trend or seasonality\n",
    "2. Double Exponential Smoothing - captures trend\n",
    "3. Triple Exponential Smoothing - captures trend & seasonality\n",
    "\n",
    "#### Single Exponential\n",
    "This method is useful if your data lacks trend and seasonality and you want to approximately track patterns in your data. Furthermore, this method removes the lag associated with the moving average techniques discussed above. \n",
    "\n",
    "#### Double Exponential \n",
    "Should your data exhibit a trend, you'll want to use this smoothing method. It has all the benefits of Single Exponential with the ability to pickup on trend. \n",
    "\n",
    "#### Triple Exponential\n",
    "Should your data exhibit trend and seasonality, you'll want to use this smoothing method. It has all the benefits of Double Exponential with the ability to pickup on seasonality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Smoothed Time Series in Python\n",
    "We now have the foundation behind us to tackle smoothing in Python. As we've done in the past, we'll leverage statsmodels to do the heavy lifting for us behind the scenes. Therefore, this section is primarily application based. \n",
    "\n",
    "We'll use the same trend and seasonality dataset throughout to compare simple average, single, double, and triple exponential smoothing. \n",
    "\n",
    "We'll holdout the last 5 samples from the dataset so we can see how each model makes predictions. We'll then compare those to actuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = trend_seasonality[:-5]\n",
    "test = trend_seasonality[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find mean of series\n",
    "trend_seasonal_avg = np.mean(trend_seasonality)\n",
    "\n",
    "# create array of mean value equal to length of time array\n",
    "simple_avg_preds = np.full(shape=len(test), fill_value=trend_seasonal_avg, dtype='float')\n",
    "\n",
    "# mse\n",
    "simple_mse = mse(test, simple_avg_preds)\n",
    "\n",
    "# results\n",
    "print(\"Predictions: \", simple_avg_preds)\n",
    "print(\"MSE: \", simple_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time[:-5], train, 'b--', label=\"train\")\n",
    "plt.plot(time[-5:], test, color='orange', linestyle=\"--\", label=\"test\")\n",
    "plt.plot(time[-5:], simple_avg_preds, 'r--', label=\"predictions\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(\"Simple Average Smoothing\")\n",
    "plt.grid(alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Details:** This is a crude model to say the least, but it's a great model to use as a baseline. In other words, any model you produce moving forward should do much better than this one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import SimpleExpSmoothing\n",
    "\n",
    "single = SimpleExpSmoothing(train).fit(optimized=True)\n",
    "single_preds = single.forecast(len(test))\n",
    "single_mse = mse(test, single_preds)\n",
    "print(\"Predictions: \", single_preds)\n",
    "print(\"MSE: \", single_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time[:-5], train, 'b--', label=\"train\")\n",
    "plt.plot(time[-5:], test, color='orange', linestyle=\"--\", label=\"test\")\n",
    "plt.plot(time[-5:], single_preds, 'r--', label=\"predictions\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(\"Single Exponential Smoothing\")\n",
    "plt.grid(alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Details:** This is certainly better than the simple average method but it's still pretty crude. Notice how the forecast is a horizontal line. Single Exponential Smoothing cannot pickup on trend or seasonality, which harkens back to mean we calculated for the stationary data way back at the beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double Exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import Holt\n",
    "\n",
    "double = Holt(train).fit(optimized=True)\n",
    "double_preds = double.forecast(len(test))\n",
    "double_mse = mse(test, double_preds)\n",
    "print(\"Predictions: \", double_preds)\n",
    "print(\"MSE: \", double_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time[:-5], train, 'b--', label=\"train\")\n",
    "plt.plot(time[-5:], test, color='orange', linestyle=\"--\", label=\"test\")\n",
    "plt.plot(time[-5:], double_preds, 'r--', label=\"predictions\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(\"Double Exponential Smoothing\")\n",
    "plt.grid(alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Details:** Double Exponential Smoothing can pickup on trend, which is exactly what we see here. This is a significant leap but no quite where we need to be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Triple Exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import ExponentialSmoothing\n",
    "\n",
    "triple = ExponentialSmoothing(train,\n",
    "                              trend=\"additive\",\n",
    "                              seasonal=\"additive\",\n",
    "                              seasonal_periods=13).fit(optimized=True)\n",
    "triple_preds = triple.forecast(len(test))\n",
    "triple_mse = mse(test, triple_preds)\n",
    "print(\"Predictions: \", triple_preds)\n",
    "print(\"MSE: \", triple_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time[:-5], train, 'b--', label=\"train\")\n",
    "plt.plot(time[-5:], test, color='orange', linestyle=\"--\", label=\"test\")\n",
    "plt.plot(time[-5:], triple_preds, 'r--', label=\"predictions\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(\"Triple Exponential Smoothing\")\n",
    "plt.grid(alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Details:** Triple Exponential Smoothing pickups trend and seasonality. This is clear in the plot above. This approach makes the most sense for this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {'MSE':[simple_mse, single_mse, double_mse, triple_mse]}\n",
    "df = pd.DataFrame(data_dict, index=['simple', 'single', 'double', 'triple'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #3\n",
    "You have been provided two datasets: \n",
    "1. **smooth_1.npy**\n",
    "2. **smooth_2.npy**\n",
    "\n",
    "Your task is to leverage what you've learned in this and previous courses. \n",
    "\n",
    "More specifically, you will do the following:\n",
    "1. Read in **smooth_1.npy** and **smooth_2.npy**.\n",
    "2. Create a time variable called **mytime** that starts at 0 and is as long as each dataset.\n",
    "3. Split each dataset into train and test sets (test set is last 5 observations).\n",
    "4. Identify trend and seasonality, if present.\n",
    "5. Identify if trend and/or seasonality are additive or multiplicative, if present.\n",
    "6. Create smoothed model on the train set and use to forecast on the test set.\n",
    "7. Calculate MSE on test data.\n",
    "8. Plot training data, test data, and your model's forecast for each dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "path_to_file = \"./\"\n",
    "smooth_1 = np.load(path_to_file + \"smooth_1.npy\")\n",
    "smooth_2 = np.load(path_to_file + \"smooth_2.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(smooth_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create mytime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time component\n",
    "mytime = np.arange(len(smooth_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 100\n",
    "# train sets\n",
    "train_1 = smooth_1[:-test_size]\n",
    "train_2 = smooth_2[:-test_size]\n",
    "\n",
    "# test sets\n",
    "test_1 = smooth_1[-test_size:]\n",
    "test_2 = smooth_2[-test_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. ID Trend / Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sequence_plot(mytime, smooth_1, title=\"smooth_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**smooth_1** has three trend components as seen above. There is no seasonality present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(smooth_2,'k')\n",
    "plt.title(\"smooth_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**smooth_2** is indicative of autocorrelation. There is no clear seasonality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. ID Additive vs Multicplicative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell from the run-sequence plots above that **smooth_1** and **smooth_2** are both additive time series. If you're not sure why, please review the lesson on additive vs multiplicative time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Create Smoothed Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import ExponentialSmoothing\n",
    "\n",
    "model_1 = ExponentialSmoothing(train_1,damped=True,\n",
    "                              trend=\"additive\",\n",
    "                              seasonal=None,\n",
    "                              seasonal_periods=None).fit(optimized=True)\n",
    "\n",
    "model_2 = ExponentialSmoothing(train_2,\n",
    "                              trend=None,\n",
    "                              seasonal=None,\n",
    "                              seasonal_periods=None).fit(optimized=True)\n",
    "\n",
    "preds_1 = model_1.forecast(len(test_2))\n",
    "preds_2 = model_2.forecast(len(test_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Calculate MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((test_1-preds_1)**2).sum()/len(test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_1 = mse(test_1, preds_1)\n",
    "mse_2 = mse(test_2, preds_2)\n",
    "print(\"MSE for smooth_1: {:.7}\".format(mse_1))\n",
    "print(\"MSE for smooth_2: {:.7}\".format(mse_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_1 = mse(test_1, preds_1)\n",
    "mse_2 = mse(test_2, preds_2)\n",
    "print(\"MSE for smooth_1: {:.7}\".format(mse_1))\n",
    "print(\"MSE for smooth_2: {:.7}\".format(mse_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Plot Train, Test, Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mytime[:-test_size], train_1, 'b--', label=\"train_1\")\n",
    "plt.plot(mytime[-test_size:], test_1, color='orange', linestyle=\"--\", label=\"test_1\")\n",
    "plt.plot(mytime[-test_size:], preds_1, 'r--', label=\"predictions\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(\"Smoothing of smooth_1 Dataset\")\n",
    "plt.grid(alpha=0.3);\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.plot(mytime[:-test_size], train_2, 'b--', label=\"train_2\")\n",
    "plt.plot(mytime[-test_size:], test_2, color='orange', linestyle=\"--\", label=\"test_2\")\n",
    "plt.plot(mytime[-test_size:], preds_2, 'r--', label=\"predictions\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(\"Triple Exponential Smoothing\")\n",
    "plt.grid(alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this assignment you should have learned: \n",
    "1. What smoothing is and why it is necessary.\n",
    "2. Several common smoothing techniques.\n",
    "3. A basic understanding of how to smooth time series data with Python and generate forecasts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Machine Learning Foundation (C) 2020 IBM Corporation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
