{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "# Machine Learning Foundation\n",
    "\n",
    "## Course 6, Part g: Time Series Deep Learning DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In previous lessons, we've gone from learning the basic components of time series to employing advanced signal transformation techniques. As the culmination of these lessons, we'll now turn toward deep learning as a method for time series forecasting. We'll see how **recurrent neural networks** can automatically learn how to model series components like seasonality and autocorrelation without special preprocessing. In the process, we'll gain exposure to the python library **Keras**, which provides an excellent interface for constructing neural networks. \n",
    "\n",
    "# Learning Outcomes\n",
    "You should walk away from this Python tutorial with:\n",
    "1. A practical understanding of how to build and train a simple RNN or LSTM for time series forecasting, using keras.\n",
    "2. Introductory experience in tuning RNN/LSTM parameters.\n",
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_config' from 'tensorflow.python.eager.context' (C:\\Users\\Vikki\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f63eeeddfff2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSimpleRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'get_config' from 'tensorflow.python.eager.context' (C:\\Users\\Vikki\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py)"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "import seaborn as sns\n",
    "#os.chdir('data')\n",
    "from colorsetup import colors, palette\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_palette(palette)\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, LSTM, Activation, Dropout\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Simple RNN\n",
    "\n",
    "In this section, we will build a recurrent neural network and train it to forecast a single time series. We'll use a dataset provided by the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/PM2.5+Data+of+Five+Chinese+Cities) that measures hourly air quality in Chinese cities/city districts<sup>1</sup>.\n",
    "\n",
    "1. Liang, X., S. Li, S. Zhang, H. Huang, and S. X. Chen (2016), PM2.5 data reliability, consistency, and air quality assessment in five Chinese cities, J. Geophys. Res. Atmos., 121, 10220â€“10236"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Up The Data\n",
    "\n",
    "We'll start by working with Beijing data, and filter the dataset down to records from 2015. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Beijing = pd.read_csv('./FiveCitiesPM/Beijing.csv')\n",
    "df_Beijing = df_Beijing[df_Beijing.year >= 2015]\n",
    "df_Beijing.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in attempting to forecast the 'PM' series, which are measurements of air pollution for several different districts.  Note that there are occasional missing values in these series, which we can fill with simple linear interpolation. To start, we'll focus on the \"PM_Dongsi\" series and interpolate the missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_Beijing['TEMP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Beijing['PM_Dongsi'] = df_Beijing['PM_Dongsi'].interpolate()\n",
    "df_Beijing['TEMP'] = df_Beijing['TEMP'].interpolate()\n",
    "df_Beijing['PM_Dongsi'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_date(row):\n",
    "    return datetime(year = row['year'], month = row['month'], day = row['day'], hour = row['hour'])\n",
    "df_Beijing['date'] = df_Beijing.apply(make_date,axis=1)\n",
    "df_Beijing.set_index(df_Beijing.date,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick plot of full time series\n",
    "plt.figure(figsize = (15,5))\n",
    "df_Beijing['PM_Dongsi'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Beijing['PM_Dongsi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, it's a good idea for us to generate a run-sequence plot before modeling the data. This way we can get a feel for what we're working with. We'll go ahead and define two utility functions that let us extract and plot the last $n$ days of data (remember that this is an hourly time series, so each day has 24 time steps).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_last_days(df, series_name, n_days):\n",
    "    \"\"\"\n",
    "    Extract last n_days of an hourly time series\n",
    "    \"\"\"\n",
    "    \n",
    "    return df[series_name][-(24*n_days):] \n",
    "\n",
    "def plot_n_last_days(df, series_name, n_days):\n",
    "    \"\"\"\n",
    "    Plot last n_days of an hourly time series \n",
    "    \"\"\"\n",
    "    plt.figure(figsize = (10,5))   \n",
    "    plt.plot(get_n_last_days(df, series_name, n_days), 'k-')\n",
    "    plt.title('{0} Air Quality Time Series - {1} days'\n",
    "              .format(series_name, n_days))\n",
    "    plt.xlabel('Recorded Hour')\n",
    "    plt.ylabel('Reading')\n",
    "    plt.grid(alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the last 6 weeks of data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_Beijing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1391a88d16cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot_n_last_days\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_Beijing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PM_Dongsi'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_Beijing' is not defined"
     ]
    }
   ],
   "source": [
    "plot_n_last_days(df_Beijing, 'PM_Dongsi', 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Review Question**: what components that you've learned in previous lessons appear to be present in this time series? \n",
    "\n",
    "**Answer**: There appears to be a periodic component as well as autocorrelation structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: Train a simple RNN to forecast the PM_Dongsi time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can train a neural network with keras, we need to process the data into a format that the library accepts. In particular, for keras RNNs and LSTMs, training samples should be stored in a 3D numpy array of shape **(n_samples, time_steps, n_features)**. Since we'll be using only the series' history to predict its future, we'll only have 1 feature. Also, for the next-step prediction that we'll do in this notebook, target values can be stored in a simple list.\n",
    "\n",
    "To this end, we define utility functions that allow us to extract the formatted data. The **get_train_test_data** function gives us the flexibility to define the length of the extracted training and test sequences and the number of time steps to use for prediction -- we'll run simple tests of our models by holding out the end of the extracted sequence and generating predictions to compare against the ground truth.\n",
    "\n",
    "Since our model will perform better with multiple training samples, we draw many slices from the entire training sequence, starting at different points in time. The gap between starting points of these slices is controlled by the **sample_gap** parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Beijing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_format_series(series):\n",
    "    \"\"\"\n",
    "    Convert a series to a numpy array of shape \n",
    "    [n_samples, time_steps, features]\n",
    "    \"\"\"\n",
    "    \n",
    "    series = np.array(series)\n",
    "    return series.reshape(series.shape[0], series.shape[1], 1)\n",
    "\n",
    "def get_train_test_data(df, series_name, series_days, input_hours, \n",
    "                        test_hours, sample_gap=3):\n",
    "    \"\"\"\n",
    "    Utility processing function that splits an hourly time series into \n",
    "    train and test with keras-friendly format, according to user-specified\n",
    "    choice of shape.    \n",
    "    \n",
    "    arguments\n",
    "    ---------\n",
    "    df (dataframe): dataframe with time series columns\n",
    "    series_name (string): column name in df\n",
    "    series_days (int): total days to extract\n",
    "    input_hours (int): length of sequence input to network \n",
    "    test_hours (int): length of held-out terminal sequence\n",
    "    sample_gap (int): step size between start of train sequences; default 5\n",
    "    \n",
    "    returns\n",
    "    ---------\n",
    "    tuple: train_X, test_X_init, train_y, test_y     \n",
    "    \"\"\"\n",
    "    \n",
    "    forecast_series = get_n_last_days(df, series_name, series_days).values # reducing our forecast series to last n days\n",
    "\n",
    "    train = forecast_series[:-test_hours] # training data is remaining days until amount of test_hours\n",
    "    test = forecast_series[-test_hours:] # test data is the remaining test_hours\n",
    "\n",
    "    train_X, train_y = [], []\n",
    "\n",
    "    # range 0 through # of train samples - input_hours by sample_gap. \n",
    "    # This is to create many samples with corresponding\n",
    "    for i in range(0, train.shape[0]-input_hours, sample_gap): \n",
    "        train_X.append(train[i:i+input_hours]) # each training sample is of length input hours\n",
    "        train_y.append(train[i+input_hours]) # each y is just the next step after training sample\n",
    "\n",
    "    train_X = get_keras_format_series(train_X) # format our new training set to keras format\n",
    "    train_y = np.array(train_y) # make sure y is an array to work properly with keras\n",
    "    \n",
    "    # The set that we had held out for testing (must be same length as original train input)\n",
    "    test_X_init = test[:input_hours] \n",
    "    test_y = test[input_hours:] # test_y is remaining values from test set\n",
    "    \n",
    "    return train_X, test_X_init, train_y, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the **get_train_test_data** utility function in hand, we're all set to extract keras-friendly arrays and start training simple RNN models. We run this function in the cell below. We use the last 56 days of the PM_Dongsi series, and will train a model that takes in 12 time steps in order to predict the next time step. We use the last day of data for visually testing the model.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_days = 56\n",
    "input_hours = 12\n",
    "test_hours = 24\n",
    "\n",
    "train_X, test_X_init, train_y, test_y = \\\n",
    "    (get_train_test_data(df_Beijing, 'PM_Dongsi', series_days, \n",
    "                         input_hours, test_hours))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we see that by taking multiple time slices, we get 436 training samples of 12 time steps each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training input shape: {}'.format(train_X.shape))\n",
    "print('Training output shape: {}'.format(train_y.shape))\n",
    "print('Test input shape: {}'.format(test_X_init.shape))\n",
    "print('Test output shape: {}'.format(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we're ready to train! Since we'd like to repeatedly adjust our model's hyperparameters to see what works best, we'll write a reusable function for training a simple RNN model using keras. Take some time to understand what the keras syntax accomplishes at each step and how it relates to what we've learned about RNNs so far.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_SimpleRNN(train_X, train_y, cell_units, epochs):\n",
    "    \"\"\"\n",
    "    Fit Simple RNN to data train_X, train_y \n",
    "    \n",
    "    arguments\n",
    "    ---------\n",
    "    train_X (array): input sequence samples for training \n",
    "    train_y (list): next step in sequence targets\n",
    "    cell_units (int): number of hidden units for RNN cells  \n",
    "    epochs (int): number of training epochs   \n",
    "    \"\"\"\n",
    "\n",
    "    # initialize model\n",
    "    model = Sequential() \n",
    "    \n",
    "    # construct an RNN layer with specified number of hidden units\n",
    "    # per cell and desired sequence input format \n",
    "    model.add(SimpleRNN(cell_units, input_shape=(train_X.shape[1],1)))\n",
    "    \n",
    "    # add an output layer to make final predictions \n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # define the loss function / optimization strategy, and fit\n",
    "    # the model with the desired number of passes over the data (epochs) \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(train_X, train_y, epochs=epochs, batch_size=64, verbose=0)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now let's use this function to fit a very simple baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fit_SimpleRNN(train_X, train_y, cell_units=10, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad so far. But we need to work a bit harder to actually extract multi-step predictions from this model, as it was trained to predict only one future time step. For multi-step forecasting, we'll iteratively generate one prediction, append it to the end of the input sequence (and shift that sequence forward by one step), then feed the new sequence back to the model. We stop once we've generated all the time step predictions we need. \n",
    "\n",
    "This prediction method and a utility function for plotting its output against the ground truth are defined below. Take some time time to familiarize yourself with the prediction method.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_init, n_steps, model):\n",
    "    \"\"\"\n",
    "    Given an input series matching the model's expected format,\n",
    "    generates model's predictions for next n_steps in the series      \n",
    "    \"\"\"\n",
    "    \n",
    "    X_init = X_init.copy().reshape(1,-1,1)\n",
    "    preds = []\n",
    "    \n",
    "    # iteratively take current input sequence, generate next step pred,\n",
    "    # and shift input sequence forward by a step (to end with latest pred).\n",
    "    # collect preds as we go.\n",
    "    for _ in range(n_steps):\n",
    "        pred = model.predict(X_init)\n",
    "        preds.append(pred)\n",
    "        X_init[:,:-1,:] = X_init[:,1:,:] # replace first 11 values with 2nd through 12th\n",
    "        X_init[:,-1,:] = pred # replace 12th value with prediction\n",
    "    \n",
    "    preds = np.array(preds).reshape(-1,1)\n",
    "    \n",
    "    return preds\n",
    "\n",
    "def predict_and_plot(X_init, y, model, title):\n",
    "    \"\"\"\n",
    "    Given an input series matching the model's expected format,\n",
    "    generates model's predictions for next n_steps in the series,\n",
    "    and plots these predictions against the ground truth for those steps \n",
    "    \n",
    "    arguments\n",
    "    ---------\n",
    "    X_init (array): initial sequence, must match model's input shape\n",
    "    y (array): true sequence values to predict, follow X_init\n",
    "    model (keras.models.Sequential): trained neural network\n",
    "    title (string): plot title   \n",
    "    \"\"\"\n",
    "    \n",
    "    y_preds = predict(test_X_init, n_steps=len(y), model=model) # predict through length of y\n",
    "    # Below ranges are to set x-axes\n",
    "    start_range = range(1, test_X_init.shape[0]+1) #starting at one through to length of test_X_init to plot X_init\n",
    "    predict_range = range(test_X_init.shape[0], test_hours)  #predict range is going to be from end of X_init to length of test_hours\n",
    "    \n",
    "    #using our ranges we plot X_init\n",
    "    plt.plot(start_range, test_X_init)\n",
    "    #and test and actual preds\n",
    "    plt.plot(predict_range, test_y, color='orange')\n",
    "    plt.plot(predict_range, y_preds, color='teal', linestyle='--')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend(['Initial Series','Target Series','Predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we've finally arrived at the time to see how our baseline model does. We can simply run the **predict_and_plot** function on the extracted test data as below, and inspect the resulting plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_and_plot(test_X_init, test_y, model,\n",
    "                 'PM Series: Test Data and Simple RNN Predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our model is badly underfit and essentially just making constant predictions. That's ok, it was a very simple baseline and trained very quickly. \n",
    "\n",
    "We can improve by making the model more expressive, **increasing cell_units**. We can also pass over the training data many more times, **increasing epochs**, giving the model more opportunity to learn the patterns in the data. We'll try that below, it takes a longer time now since our training is more extensive.\n",
    "\n",
    "Note that there is a significant amount of randomness in neural network training - we may need to retrain the model a few times in order to get\n",
    "results that we're happy with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fit_SimpleRNN(train_X, train_y, cell_units=30, epochs=1200)\n",
    "predict_and_plot(test_X_init, test_y, model,\n",
    "                 'PM Series: Test Data and Simple RNN Predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can definitely get better results than before. Note that the model has the capacity to forecast an upward trend based on the trough pattern that occured recently (the input sequence). \n",
    "\n",
    "Once we've created a model object, we can also get information about its structure and number of parameters by using the **summary** function. This is a useful way to measure the complexity of the model and get a feel for how long it may take to train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even for this relatively simple model, we already have almost a thousand parameters to train. A larger number of cell units would increase the number of parameters - this is why the training process can become so time consuming.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1: Train a simple RNN to forecast the PM_Nongzhanguan time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reuse all of the functions we've defined so far in order to train models on different time series. In this exercise, you'll train your own model to forecast the \"PM_Nongzhanguan\" series from the Beijing dataframe.\n",
    "\n",
    "**Step 1**: Interpolate the missing values in the \"PM_Nongzhanguan\" series and plot the last 42 days of the series to get a feel for the data. \n",
    "* We'll do this step together to get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Beijing['PM_Nongzhanguan'] = df_Beijing['PM_Nongzhanguan'].interpolate()\n",
    "df_Beijing['PM_Nongzhanguan'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_n_last_days(df_Beijing, 'PM_Nongzhanguan', 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Extract the train and test data for the \"PM_Nongzhanguan\" series using the function **get_train_test_data**. Use the following set of parameters:  \n",
    "\n",
    "* **series_days** : 56\n",
    "* **input_hours** : 6\n",
    "* **test_hours** : 12\n",
    "\n",
    "For reference, below is how we called the function earlier on. You can also pull up the function's documentation to review the various arguments. \n",
    "\n",
    "```\n",
    "series_days = 56\n",
    "input_hours = 12\n",
    "test_hours = 24\n",
    "\n",
    "train_X, test_X_init, train_y, test_y = \\\n",
    "    (get_train_test_data(df_Beijing, 'PM_Dongsi', series_days, \n",
    "                         input_hours, test_hours))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_days = 56\n",
    "input_hours = 6\n",
    "test_hours = 12\n",
    "\n",
    "train_X, test_X_init, train_y, test_y = \\\n",
    "    (get_train_test_data(df_Beijing, 'PM_Nongzhanguan', series_days, \n",
    "                         input_hours, test_hours))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Using the extracted train data to fit a simple RNN, and use the test data to generate and plot predictions.\n",
    "\n",
    "* Start with a simple baseline -- few cell units and epochs. From here, try to make the model more expressive by increasing units and epochs until you're satisfied with the model's predictions. \n",
    "\n",
    "* Be careful not to set units and/or epochs *too* high. The model may become very slow to train and also start to badly overfit the training data with the extra complexity you've added.\n",
    "\n",
    "For reference, here's example code that you can adapt:\n",
    "```\n",
    "model = fit_SimpleRNN(train_X, train_y, cell_units=10, epochs=10)\n",
    "\n",
    "predict_and_plot(test_X_init, test_y, model,\n",
    "                 'PM Series: Test Data and Simple RNN Predictions')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fit_SimpleRNN(train_X, train_y, cell_units=30, epochs=1200)\n",
    "\n",
    "predict_and_plot(test_X_init, test_y, model, \n",
    "                 'PM_Nongzhanguan Series: Test Data and Simple RNN Predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we're able to do a decent job forecasting the continuation of an uptrend. We'll likely face more difficulty if we try to predict further into the future, especially with a simple RNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll build on our previous work by introducing LSTM models as an enhancement to the RNNs we've trained so far. Our first step will be to write a new function for fitting an LSTM with keras - notice that it's almost the same as our simple RNN function, with **LSTM** substitued for **SimpleRNN** (this is a nice display of how flexible keras is). \n",
    "\n",
    "Take some time to review the logic of the function while we go ahead and run the example cell below (it will take a while). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_LSTM(train_X, train_y, cell_units, epochs):\n",
    "    \"\"\"\n",
    "    Fit LSTM to data train_X, train_y \n",
    "    \n",
    "    arguments\n",
    "    ---------\n",
    "    train_X (array): input sequence samples for training \n",
    "    train_y (list): next step in sequence targets\n",
    "    cell_units (int): number of hidden units for LSTM cells  \n",
    "    epochs (int): number of training epochs   \n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize model\n",
    "    model = Sequential() \n",
    "    \n",
    "    # construct a LSTM layer with specified number of hidden units\n",
    "    # per cell and desired sequence input format \n",
    "    model.add(LSTM(cell_units, input_shape=(train_X.shape[1],1))) #,return_sequences= True))\n",
    "    #model.add(LSTM(cell_units_l2, input_shape=(train_X.shape[1],1)))\n",
    "    \n",
    "    # add an output layer to make final predictions \n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # define the loss function / optimization strategy, and fit\n",
    "    # the model with the desired number of passes over the data (epochs) \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(train_X, train_y, epochs=epochs, batch_size=64, verbose=0)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: Train a LSTM to forecast the PM_Nongzhanguan time series\n",
    "\n",
    "With our new LSTM training function and all of our previously defined utility functions, adapting our code for LSTM forecasting will be fairly simple. We can extract the data as we did before, call the **fit_LSTM** function to build a model, and run the same *predict_and_plot* code.\n",
    "\n",
    "Remember that one of the key benefits of LSTMs over simple RNNs is that they are better equipped to handle long input sequences and long-term dependencies. To see this evidence of this, we'll set *input_hours* to 12 and *test_hours* to 96 and see how our model predictions turn out with LSTM.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_days = 50 \n",
    "input_hours = 12\n",
    "test_hours = 96\n",
    "\n",
    "train_X, test_X_init, train_y, test_y = \\\n",
    "    (get_train_test_data(df_Beijing, 'PM_Nongzhanguan', series_days, \n",
    "                         input_hours, test_hours))\n",
    "\n",
    "model = fit_LSTM(train_X, train_y, cell_units=70, epochs=3000) \n",
    "\n",
    "predict_and_plot(test_X_init, test_y, model, \n",
    "                 'PM_Nongzhanguan Series: Test Data and LSTM Predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our prediction plot we can start to see how LSTMs can be more expressive than simple RNNs - instead of just extrapolating a simple trend like our previous RNN models did, this LSTM model can effectively anticipate inflection points.\n",
    "\n",
    "You should also notice that our model starts to struggle toward the end of the predicted sequence, becoming more conservative in its predictions. To improve the quality of forecasts over many time steps, we'd likely need to use more data and more sophisticated LSTM model structures that are beyond the scope of this lesson.\n",
    "\n",
    "Take a look at the model summary and compare it with the summary for our simple RNN from example 1. You can see that there are many more trainable parameters for the LSTM, which explains why it took a much longer time for us to train this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further Exploration\n",
    "\n",
    "The simple models we've worked with are only the tip of the iceberg for deep learning. We've been time-limited for this exercise, and typical deep learning models involve much longer training times than what we're able to do in this notebook. \n",
    "\n",
    "Here are several suggestions for how you could explore these ideas further, leveraging the code we've implemented today:\n",
    "\n",
    "* Try using longer chunks of the series we've looked at in this notebook for modeling (set series_days larger than 56), or modeling other series in the dataset.\n",
    "* When training with more data, try increasing cell_units and running more training epochs.  \n",
    "* Try using longer input sequences with LSTM, and predicting a wider range of test hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we have covered:\n",
    "\n",
    "1. How recurrent neural networks can be applied to sequence forecasting problems\n",
    "2. How simple RNNs and LSTMs can be built and trained using the python library keras\n",
    "3. The importance of tuning network parameters, and an introductory strategy for doing so\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Machine Learning Foundation (C) 2020 IBM Corporation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
